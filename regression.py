#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon May 11 17:31:36 2020

@author: rhonadigan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# set the seed
np.random.seed(42)


def MSEStep(X, y, W, b, learn_rate = 0.005):
    """
    This function implements the gradient descent step to minimize squared error
    
    It returns W_new - the predictor feature coefficients and b_new - the intercept 
    """
    
    #compute predicted y 
    y_pred = np.matmul(X, W) + b
    
    #compute the error the these predictions
    error = y - y_pred
    
    # compute steps
    W_new = W + learn_rate * np.matmul(error, X)
    b_new = b + learn_rate * error.sum()
    
    return W_new, b_new


def miniBatchGD(X, y, batch_size = 50, learn_rate = 0.005, num_iter = 100):
    """
    This function performs mini-batch gradient descent on a given dataset.
   
    It returns all the regression coeficients and intercepts generated by the gradient descent
    """
    #set all coefficients initially to zero
    n_points = X.shape[0]
    W = np.zeros(X.shape[1]) # coefficients
    b = 0 # intercept
    
    # run iterations
    regression_coef = [np.hstack((W,b))]
    
    # run the gradient descent process multiple times to optimize the parameters
    for _ in range(num_iter):
        batch = np.random.choice(range(n_points), batch_size)
        
        X_batch = X[batch,:]
        y_batch = y[batch]
        
        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)
        regression_coef.append(np.hstack((W,b)))
    
    return regression_coef


if __name__ == "__main__":
    
    #read in the data and examine the shape of it
    data = pd.read_csv(r'/Users/rhonadigan/Downloads/MolocoQ2.csv',names=['A','B','C'])
    data.describe()
    
    #normalise the data
    data = (data - data.mean())/data.std()
    data.head()
    
    # split the predictor variables and feature variable
    X = data.loc[:,'A':'B'].to_numpy()
    y = data.loc[:,'C'].to_numpy()
       
    #run the regression and get the coefficients
    regression_coef = miniBatchGD(X, y)
    
    # get the best coefficient/intercepts (the last ones in the gradient descent)
    coefs = regression_coef[-1][0:2]
    intercept = regression_coef[-1][2]
    
    # plot the results
    fig = plt.figure()
    ax = fig.add_subplot(111,projection='3d')
    
    #create scatter plot of original data
    ax.scatter(X[:,0], X[:,1], y, marker='.', color='red')
    
    #set axes labels
    ax.set_xlabel("A")
    ax.set_ylabel("B")
    ax.set_zlabel("C")
    
    #set axes limits
    ax.set(xlim=(2, -2), ylim=(2, -2), zlim=(2,-2))

    # set up plane space
    xs = np.linspace(-2, 2, 30)
    ys = np.linspace(-2, 2, 30)
    xs, ys = np.meshgrid(xs, ys)
    
    #create the regression equation
    zs = xs*coefs[0]+ys*coefs[1]+intercept
    
    #plot the plane of best fit and show the graph
    ax.plot_surface(xs,ys,zs, alpha=0.5)
    plt.show()
        
    #print the equation with coefs to 3 decimal places
    print("Equation: C = {:.3f} + {:.3f}A + {:.3f}B".format(intercept, coefs[0],
                                                              coefs[1]))
    

